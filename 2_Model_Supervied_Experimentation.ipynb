{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4930a224-5d95-4f92-876f-024dc9f1dbd8",
   "metadata": {},
   "source": [
    "1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade7a71-868b-43f7-942d-7f0bf3a24e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from typing import List, Dict\n",
    "\n",
    "# Add project root to path\n",
    "project_root = str(Path.cwd().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data.data_processing import DataProcessor  # For reference if needed\n",
    "from src.models.model_training import ModelTrainer\n",
    "\n",
    "# Load base configuration\n",
    "with open('configs/config.yml', 'r') as f:\n",
    "    base_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"[INFO] Imports and base configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192c66c-9f98-452a-bb2c-1fc78b12b443",
   "metadata": {},
   "source": [
    "2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799d636-ec04-422c-84b3-9b0e408e5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Processed Data\n",
    "\n",
    "processed_data_path = base_config['data']['processed_data_path']\n",
    "raw_data_file_name = base_config['data']['raw_data_file_name']\n",
    "\n",
    "# NOTE: This CSV file is ALREADY transformed via DataProcessor\n",
    "df = pd.read_csv(f\"{processed_data_path}processed_{raw_data_file_name}\")\n",
    "print(\"[INFO] Processed training data loaded.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# Identify target column\n",
    "target_column_name = base_config['data']['raw_data_target_column']['name']\n",
    "if base_config['data']['raw_data_target_column']['type'] == 'categorical':\n",
    "    target_column_name = f'{target_column_name}_categorical_encoded'\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=[target_column_name])\n",
    "y = df[target_column_name]\n",
    "\n",
    "# Store feature names\n",
    "training_features = X.columns.tolist()\n",
    "print(\"Number of training features:\", len(training_features))\n",
    "print(\"First few features:\", training_features[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcaee1-b7c7-4bba-8aea-50fa944c6d2d",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c79e0-5ca3-47ff-8090-7725e8387c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_experiment_comparisons(results):\n",
    "    # Extract metrics for comparison\n",
    "    experiment_names = [r['experiment_name'] for r in results]\n",
    "    accuracies = [r['metrics'].get('accuracy', 0) for r in results]\n",
    "    f1_scores = [r['metrics'].get('f1', 0) for r in results]\n",
    "\n",
    "    # Plot comparisons\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(experiment_names, accuracies)\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(experiment_names, f1_scores)\n",
    "    plt.title('F1 Score Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa9d8c-8b92-4ccf-861a-359e9d0816a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper Functions (2) - validate_model_stability\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def validate_model_stability(trainer, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Validate model stability using cross-validation.\n",
    "    \"\"\"\n",
    "    model = trainer.model.model  # Access the underlying sklearn estimator\n",
    "    cv_scores = cross_val_score(model, X, y, cv=n_splits)\n",
    "\n",
    "    print(\"\\nCross-validation Results - validate_model_stability(...) :\")\n",
    "    print(f\"Individual scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be9b08-eca2-4776-b1b7-fa9777bd0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Helper Functions (3) - analyze_important_words\n",
    "\n",
    "def analyze_important_words(trainer, feature_columns: List[str], top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Analyze and display most important features (words) for prediction.\n",
    "    \"\"\"\n",
    "    model = trainer.model.model\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(\"[WARN] Model doesn't have feature_importances_ attribute.\")\n",
    "        return {}\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    importance_dict = dict(zip(feature_columns, importances))\n",
    "\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = sorted_features[:top_n]\n",
    "\n",
    "    print(\"\\nTop Important Features:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Feature Name\".ljust(30) + \"Importance\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for feature_name, importance in top_features:\n",
    "        clean_name = feature_name.replace('tweet_feature_', '')\n",
    "        print(f\"{clean_name.ljust(30)} {importance:.4f}\")\n",
    "\n",
    "    return dict(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba25b5-a6d4-4c56-9004-9c851b0cd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_importance(importance_dict, figsize=(12, 6)):\n",
    "    if not importance_dict:\n",
    "        return  # no-op if the model has no importances\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    features = [k.replace('tweet_feature_', '') for k in importance_dict.keys()]\n",
    "    values = list(importance_dict.values())\n",
    "\n",
    "    y_pos = np.arange(len(features))\n",
    "    plt.barh(y_pos, values)\n",
    "    plt.yticks(y_pos, features)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec939c-469e-4e41-8c74-c23f9418d2fe",
   "metadata": {},
   "source": [
    "3. Experiment: Different Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727b1fd-d61f-4f49-9122-bf2ef58de162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run Experiment with Different Model Configurations\n",
    "\n",
    "def run_experiment(config_variations, X, y):\n",
    "    \"\"\"Run experiments with different model configurations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for variant in config_variations:\n",
    "        print('\\n========== Start evaluation:', variant['name'], '==========\\n')\n",
    "        \n",
    "        # Create new config for this experiment\n",
    "        experiment_config = base_config.copy()\n",
    "        experiment_config['training']['model']['params'].update(variant['params'])\n",
    "        \n",
    "        # Train model\n",
    "        trainer = ModelTrainer(experiment_config)\n",
    "        \n",
    "        # Store training features in trainer (for reference if saving later)\n",
    "        trainer.training_features = training_features\n",
    "        \n",
    "        metrics = trainer.train_model(X, y)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'experiment_name': variant['name'],\n",
    "            'params': variant['params'],\n",
    "            'metrics': metrics,\n",
    "            'trainer': trainer,\n",
    "            'training_features': training_features\n",
    "        })\n",
    "\n",
    "        # Analyze model stability and features\n",
    "        cv_scores = validate_model_stability(trainer, X, y, n_splits=5)\n",
    "        feature_columns = [col for col in X.columns if col.startswith('tweet_feature_')]\n",
    "        importance_dict = analyze_important_words(trainer, feature_columns, top_n=15)\n",
    "        plot_feature_importance(importance_dict)\n",
    "\n",
    "        print(f\"\\nExperiment: {variant['name']}\")\n",
    "        print(\"Parameters:\", variant['params'])\n",
    "        print(\"Metrics:\", metrics)\n",
    "        print(f\"CV mean: {cv_scores.mean():.4f}, CV std: {cv_scores.std():.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define experiment configurations\n",
    "config_variations = [\n",
    "    {\n",
    "        'name': 'Deep Trees Model',\n",
    "        'params': {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 25,\n",
    "            'min_samples_split': 2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Text Optimized Model',\n",
    "        'params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 15,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'max_features': 'sqrt',\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Deep Ensemble',\n",
    "        'params': {\n",
    "            'n_estimators': 900,\n",
    "            'max_depth': 20,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,\n",
    "            'max_features': 'log2',\n",
    "            'bootstrap': True,\n",
    "            'class_weight': 'balanced_subsample'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "experiment_results = run_experiment(config_variations, X, y)\n",
    "\n",
    "# Analyze Results\n",
    "plot_experiment_comparisons(experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0770148-2364-4b88-a268-0abda8357919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_result = max(experiment_results, key=lambda x: x['metrics']['accuracy'])\n",
    "best_trainer = best_result['trainer']\n",
    "\n",
    "print(\"[INFO] Best model found:\", best_result['experiment_name'])\n",
    "print(\"Parameters:\", best_result['params'])\n",
    "print(\"Metrics:\", best_result['metrics'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea267da8-062d-4bb6-9e89-4401fab87528",
   "metadata": {},
   "source": [
    "TEST PREDICTION WITH DATA TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870aeb6b-4e27-47db-a79c-63ed376c4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate on Test Data - Function\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_on_test_data(trainer, data_test_preprocessed_df):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "\n",
    "    Args:\n",
    "        trainer: Trained ModelTrainer instance\n",
    "        data_test_preprocessed_df: Preprocessed test DataFrame\n",
    "    \"\"\"\n",
    "    # Prepare test features and target\n",
    "    target_column = trainer.config['data']['raw_data_target_column']['name']\n",
    "    if trainer.config['data']['raw_data_target_column']['type'] == 'categorical':\n",
    "        target_column = f'{target_column}_categorical_encoded'\n",
    "\n",
    "    X_test = data_test_preprocessed_df.drop(columns=[target_column])\n",
    "    y_test = data_test_preprocessed_df[target_column]\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = trainer.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix on Test Data')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Compare train vs test performance\n",
    "    print(\"\\nTrain vs Test Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    train_metrics = trainer.training_history[-1]['metrics'] if trainer.training_history else {}\n",
    "    for metric in test_metrics.keys():\n",
    "        train_value = train_metrics.get(metric, 0)\n",
    "        test_value = test_metrics[metric]\n",
    "        diff = abs(train_value - test_value)\n",
    "        print(f\"{metric}:\")\n",
    "        print(f\"  Train: {train_value:.4f}\")\n",
    "        print(f\"  Test:  {test_value:.4f}\")\n",
    "        print(f\"  Diff:  {diff:.4f}\")\n",
    "\n",
    "    return test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d02f4-ec89-4544-95d2-f2da150dbdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Load Processed Test Data & Evaluate\n",
    "\n",
    "import os\n",
    "file_name, file_extension = os.path.splitext(raw_data_file_name)\n",
    "test_file_name = f\"{file_name}_test{file_extension}\"\n",
    "test_df = pd.read_csv(f\"{processed_data_path}processed_{test_file_name}\")\n",
    "print(\"[INFO] Processed test data loaded.\")\n",
    "print(\"Shape:\", test_df.shape)\n",
    "\n",
    "test_metrics = evaluate_on_test_data(best_trainer, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ffbb3-86f0-479a-a243-c2a0cb4890d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training and selecting best model, let see the prediction with data test\n",
    "test_metrics = evaluate_on_test_data(best_trainer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9bf36-6579-43db-93f2-ed87f2b31df3",
   "metadata": {},
   "source": [
    "SAVE THE BEST ALGORITH BASE ON OUR PREVIOUS EXPERIME?TATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bfba6-8f4d-4c6e-84cd-5f8376d076a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save the Best Model\n",
    "\n",
    "def save_best_model(experiment_results):\n",
    "    \"\"\"Save the best performing model with feature information.\"\"\"\n",
    "    best_result = max(experiment_results, key=lambda x: x['metrics']['accuracy'])\n",
    "    best_trainer = best_result['trainer']\n",
    "\n",
    "    relative_path_name = base_config['model']['name']\n",
    "    relative_path_version = base_config['model']['version']\n",
    "    model_saved_relative_path_directory = f\"models/{relative_path_name}/{relative_path_version}\"\n",
    "    model_saved_name = best_result['experiment_name']\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_saved_name = f\"{model_saved_name}_{timestamp}\"\n",
    "\n",
    "    best_trainer.training_features = best_result['training_features']\n",
    "    best_trainer.save_model(model_saved_relative_path_directory, model_saved_name)\n",
    "\n",
    "    print(f\"\\n[INFO] Best Model Details:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Model Name: {best_result['experiment_name']}\")\n",
    "    print(f\"Parameters: {best_result['params']}\")\n",
    "    print(f\"Metrics: {best_result['metrics']}\")\n",
    "    print(f\"Number of features: {len(best_result['training_features'])}\")\n",
    "    print(f\"\\nModel saved to: {model_saved_relative_path_directory}/{model_saved_name}\")\n",
    "    \n",
    "    return f\"{model_saved_relative_path_directory}/{model_saved_name}\", best_trainer, best_result['metrics']\n",
    "\n",
    "model_path, final_best_trainer, final_metrics = save_best_model(experiment_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9ba27-c846-40d2-977d-c84b43b35b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model(experiment_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
