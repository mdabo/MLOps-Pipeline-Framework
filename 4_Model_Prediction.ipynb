{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53530f4c-22a3-40f4-a8a7-339e4bca4a7f",
   "metadata": {},
   "source": [
    "1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be499b65-91ee-41ba-ac83-e107e22908c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path.cwd().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import your modules\n",
    "from src.data.data_processing import DataProcessor\n",
    "from src.models.model_training import ModelTrainer\n",
    "import yaml\n",
    "\n",
    "# Load configuration\n",
    "with open('configs/config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Global variables\n",
    "trainer = None\n",
    "data_processor = None\n",
    "\n",
    "# Initialize NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Warning during NLTK resources download: {e}\")\n",
    "\n",
    "# Initialize data processor with TF-IDF configuration\n",
    "data_processor = DataProcessor('configs/config.yml')\n",
    "\n",
    "# Configure vectorizer from config\n",
    "text_features_config = config.get('features', {}).get('text_features', {})\n",
    "data_processor.vectorizer = TfidfVectorizer(\n",
    "    max_features=text_features_config.get('max_features', 50),\n",
    "    min_df=text_features_config.get('min_df', 0.02),\n",
    "    max_df=text_features_config.get('max_df', 0.90),\n",
    "    ngram_range=(1, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f68e7-b148-4684-8af5-f3f41bb4b4f5",
   "metadata": {},
   "source": [
    "2. Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf70dbc-f802-49a5-a359-8cc1eb3f8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model type: classifier\n",
      "Loading learning type: supervised\n",
      "\n",
      " MODEL FACTORY: Random Forest Classifier will be used for training with the following parameters: \n",
      "{'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
      "Model and vectorizer loaded successfully from: models/example_model/0.1.0/Deep Trees Model_20250113_210714\n",
      "Model type: classifier\n",
      "Learning type: supervised\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Saved Model\n",
    "def load_saved_model(model_path: str, config: Dict) -> ModelTrainer:\n",
    "    \"\"\"\n",
    "    Load a saved model and its associated vectorizer state.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        ModelTrainer: Initialized and loaded model trainer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model data\n",
    "        import joblib\n",
    "        model_data = joblib.load(model_path)\n",
    "        \n",
    "        print(\"Available keys in model_data:\", list(model_data.keys()))\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = ModelTrainer(config)\n",
    "        \n",
    "        # Initialize model type\n",
    "        from src.models.model_factory import ModelFactory, LearningType\n",
    "        \n",
    "        print(f\"Loading model type: {model_data['model_type']}\")\n",
    "        print(f\"Loading learning type: {model_data['learning_type']}\")\n",
    "        \n",
    "        if model_data['model_type'] == \"classifier\":\n",
    "            trainer.learning_type = LearningType.SUPERVISED_CLASSIFICATION\n",
    "            trainer.model = ModelFactory.create_model(\n",
    "                LearningType.SUPERVISED_CLASSIFICATION, \n",
    "                config\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_data['model_type']}\")\n",
    "        \n",
    "        # Load model state\n",
    "        trainer.model.model = model_data['model']\n",
    "        trainer.model.config = model_data['config']\n",
    "        trainer.model.training_history = model_data['training_history']\n",
    "        trainer.model.is_fitted = model_data['is_fitted']\n",
    "        \n",
    "        # Store training features if available\n",
    "        if 'training_features' in model_data:\n",
    "            trainer.training_features = model_data['training_features']\n",
    "            print(f\"Loaded {len(trainer.training_features)} training features\")\n",
    "        \n",
    "        print(f\"Model loaded successfully from: {model_path}\")\n",
    "        return trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model details:\")\n",
    "        print(f\"Model path: {model_path}\")\n",
    "        if 'model_data' in locals():\n",
    "            print(f\"Model data keys: {model_data.keys()}\")\n",
    "        raise\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    MODEL_PATH = \"models/example_model/0.1.0/Deep Trees Model_20250113_230521_20250113_230521\"\n",
    "    trainer = load_saved_model(MODEL_PATH, config)\n",
    "    print(\"Model type:\", trainer.model.model_type)\n",
    "    print(\"Learning type:\", trainer.model.learning_type)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910ea3f-979e-411a-af33-21fffbbec568",
   "metadata": {},
   "source": [
    "3. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2bc7927-f879-4a42-9a51-f1453c39a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Prediction Function\n",
    "def predict_sentiments(data: List[Dict], data_processor: DataProcessor, trainer: ModelTrainer) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict sentiments for new tweets using consistent feature processing.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing tweet data\n",
    "        data_processor: Initialized DataProcessor\n",
    "        trainer: Trained ModelTrainer with loaded model\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with predictions added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert input to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Process data ensuring consistent features\n",
    "        processed_df = data_processor.process_data(\n",
    "            df, \n",
    "            data_file_name='NO_TO_SAVE', \n",
    "            is_prediction=True\n",
    "        )\n",
    "        \n",
    "        # Get numeric and categorical encoded columns\n",
    "        feature_columns = processed_df.columns[\n",
    "            (processed_df.dtypes != 'object') | \n",
    "            (processed_df.columns.str.endswith('_categorical_encoded'))\n",
    "        ].tolist()\n",
    "        \n",
    "        # Remove target column if present\n",
    "        if 'sentiment_categorical_encoded' in feature_columns:\n",
    "            feature_columns.remove('sentiment_categorical_encoded')\n",
    "        \n",
    "        # Organize features in the same order as training\n",
    "        if hasattr(trainer, 'training_features'):\n",
    "            common_features = [f for f in trainer.training_features if f in feature_columns]\n",
    "            if not common_features:\n",
    "                raise ValueError(\"No common features found between training and prediction data\")\n",
    "            X = processed_df[common_features]\n",
    "        else:\n",
    "            X = processed_df[feature_columns]\n",
    "        \n",
    "        print(\"Using features for prediction:\", X.columns.tolist())\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = trainer.predict(X)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, item in enumerate(data):\n",
    "            result = item.copy()\n",
    "            result['predicted_sentiment'] = predictions[i]\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error during prediction processing:\")\n",
    "        print(f\"Available features: {processed_df.columns.tolist()}\")\n",
    "        raise\n",
    "\n",
    "# Test data\n",
    "test_payload = [\n",
    "    {\n",
    "        \"tweet_id\": 123456789,\n",
    "        \"from_platform\": \"Nvidia\",\n",
    "        \"tweet\": \"NVIDIA still the big boss of hardware AI technologies\"\n",
    "    },\n",
    "    {\n",
    "        \"tweet_id\": 987654321,\n",
    "        \"from_platform\": \"Nvidia\",\n",
    "        \"tweet\": \"What's going wrong with this firm. They are producing a bullshit\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84bb19-f127-4cef-ab0c-8ef7ae6f082d",
   "metadata": {},
   "source": [
    "4. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cccf85f-c4a5-4094-8dce-27168941bbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: missing values in numeric columns\n",
       " tweet_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: missing values in numeric columns\n",
       " tweet_id    \u001b[1;36m0\u001b[0m\n",
       "dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: Missing values in numeric columns after treatment \n",
       " tweet_id    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "dtype: int64\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: Missing values in numeric columns after treatment \n",
       " tweet_id    \u001b[1;36m0\u001b[0m\n",
       "dtype: int64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: categorical_columns\n",
       " <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Index</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'from_platform'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'tweet'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: categorical_columns\n",
       " \u001b[1;35mIndex\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'from_platform'\u001b[0m, \u001b[32m'tweet'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[32m'object'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: Data after handling missing values\n",
       "     tweet_id from_platform                                              tweet\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">123456789</span>        Nvidia  NVIDIA still the big boss of hardware AI techn<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">987654321</span>        Nvidia  What's going wrong with this firm. They are pr<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: Data after handling missing values\n",
       "     tweet_id from_platform                                              tweet\n",
       "\u001b[1;36m0\u001b[0m  \u001b[1;36m123456789\u001b[0m        Nvidia  NVIDIA still the big boss of hardware AI techn\u001b[33m...\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m  \u001b[1;36m987654321\u001b[0m        Nvidia  What's going wrong with this firm. They are pr\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mdabo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mdabo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mdabo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mdabo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS <span style=\"font-weight: bold\">(</span>vectorization<span style=\"font-weight: bold\">)</span>: Traitement de la colonne <span style=\"color: #008000; text-decoration-color: #008000\">'tweet'</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS \u001b[1m(\u001b[0mvectorization\u001b[1m)\u001b[0m: Traitement de la colonne \u001b[32m'tweet'\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS <span style=\"font-weight: bold\">(</span>vectorization<span style=\"font-weight: bold\">)</span>: Étape <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Nettoyage et normalisation du texte<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS \u001b[1m(\u001b[0mvectorization\u001b[1m)\u001b[0m: Étape \u001b[1;36m1\u001b[0m: Nettoyage et normalisation du texte\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS <span style=\"font-weight: bold\">(</span>vectorization<span style=\"font-weight: bold\">)</span>: Étape <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Vectorisation du texte<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS \u001b[1m(\u001b[0mvectorization\u001b[1m)\u001b[0m: Étape \u001b[1;36m2\u001b[0m: Vectorisation du texte\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DATA PROCESS <span style=\"font-weight: bold\">(</span>vectorization<span style=\"font-weight: bold\">)</span>: Traitement de <span style=\"color: #008000; text-decoration-color: #008000\">'tweet'</span> terminé:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DATA PROCESS \u001b[1m(\u001b[0mvectorization\u001b[1m)\u001b[0m: Traitement de \u001b[32m'tweet'\u001b[0m terminé:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- Temps de traitement: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.23</span> secondes\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- Temps de traitement: \u001b[1;36m1.23\u001b[0m secondes\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- Nombre de features créées: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- Nombre de features créées: \u001b[1;36m22\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- Exemples de features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'ai'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'ai technology'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'big'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'big bos'</span> <span style=\"color: #008000; text-decoration-color: #008000\">'bos'</span><span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- Exemples de features: \u001b[1m[\u001b[0m\u001b[32m'ai'\u001b[0m \u001b[32m'ai technology'\u001b[0m \u001b[32m'big'\u001b[0m \u001b[32m'big bos'\u001b[0m \u001b[32m'bos'\u001b[0m\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS <span style=\"font-weight: bold\">(</span>vectorization<span style=\"font-weight: bold\">)</span>: Traitement terminé!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS \u001b[1m(\u001b[0mvectorization\u001b[1m)\u001b[0m: Traitement terminé!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total des features créées: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total des features créées: \u001b[1;36m22\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dimensions finales du DataFrame: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dimensions finales du DataFrame: \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m24\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: number of numeric columns for scalling\n",
       " <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: number of numeric columns for scalling\n",
       " \u001b[1;36m23\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: Unique values for categorial feature from_platform: \n",
       " <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Nvidia'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: Unique values for categorial feature from_platform: \n",
       " \u001b[1m[\u001b[0m\u001b[32m'Nvidia'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "DATA PROCESS: Categorial encoded_data: \n",
       "    from_platform_categorical_encoded\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                                  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "DATA PROCESS: Categorial encoded_data: \n",
       "    from_platform_categorical_encoded\n",
       "\u001b[1;36m0\u001b[0m                                  \u001b[1;36m0\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m                                  \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during prediction processing:\n",
      "Available features: ['tweet_id', 'tweet_feature_ai', 'tweet_feature_ai technology', 'tweet_feature_big', 'tweet_feature_big bos', 'tweet_feature_bos', 'tweet_feature_bos hardware', 'tweet_feature_bullshit', 'tweet_feature_firm', 'tweet_feature_firm producing', 'tweet_feature_going', 'tweet_feature_going wrong', 'tweet_feature_hardware', 'tweet_feature_hardware ai', 'tweet_feature_nvidia', 'tweet_feature_nvidia still', 'tweet_feature_producing', 'tweet_feature_producing bullshit', 'tweet_feature_still', 'tweet_feature_still big', 'tweet_feature_technology', 'tweet_feature_wrong', 'tweet_feature_wrong firm', 'from_platform_categorical_encoded']\n",
      "Prediction error: The feature names should match those that were passed during fit.\n",
      "Feature names unseen at fit time:\n",
      "- tweet_feature_ai\n",
      "- tweet_feature_ai technology\n",
      "- tweet_feature_big\n",
      "- tweet_feature_big bos\n",
      "- tweet_feature_bos\n",
      "- ...\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- tweet_feature_amazon\n",
      "- tweet_feature_back\n",
      "- tweet_feature_best\n",
      "- tweet_feature_ca\n",
      "- tweet_feature_day\n",
      "- ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Make Predictions\n",
    "try:\n",
    "    results = predict_sentiments(test_payload, data_processor, trainer)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    for result in results:\n",
    "        print(f\"\\nTweet ID: {result['tweet_id']}\")\n",
    "        print(f\"Platform: {result['from_platform']}\")\n",
    "        print(f\"Tweet: {result['tweet']}\")\n",
    "        print(f\"Predicted Sentiment: {result['predicted_sentiment']}\")\n",
    "        print(\"-\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"Prediction error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d90bf5-35df-4e78-89c2-8f2c909bb515",
   "metadata": {},
   "source": [
    "5. Utility Functions for Different Input Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21f1d7-e47f-440a-8d4b-4c3cea8b5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Utility Functions\n",
    "def load_json_from_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load test data from JSON file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def predict_from_json_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Make predictions from JSON file\"\"\"\n",
    "    data = load_json_from_file(file_path)\n",
    "    return predict_sentiments(data, data_processor, trainer)\n",
    "\n",
    "def predict_single_tweet(tweet_text: str, platform: str = \"Unknown\") -> Dict:\n",
    "    \"\"\"Make prediction for a single tweet\"\"\"\n",
    "    data = [{\n",
    "        \"tweet_id\": int(time.time()),\n",
    "        \"from_platform\": platform,\n",
    "        \"tweet\": tweet_text\n",
    "    }]\n",
    "    results = predict_sentiments(data, data_processor, trainer)\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec2d04-e548-4450-a76f-008fe68fa3fa",
   "metadata": {},
   "source": [
    "6. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e33e7-0c57-45d8-8b21-01f905f77ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Example Usage\n",
    "try:\n",
    "    # 1. Predict from JSON string\n",
    "    json_str = '''\n",
    "    [\n",
    "        {\n",
    "            \"tweet_id\": 123456789,\n",
    "            \"from_platform\": \"Nvidia\",\n",
    "            \"tweet\": \"NVIDIA still the big boss of hardware AI technologies\"\n",
    "        },\n",
    "        {\n",
    "            \"tweet_id\": 987654321,\n",
    "            \"from_platform\": \"Nvidia\",\n",
    "            \"tweet\": \"What's going wrong with this firm. They are producing a bullshit\"\n",
    "        }\n",
    "    ]\n",
    "    '''\n",
    "    test_data = json.loads(json_str)\n",
    "    results = predict_sentiments(test_data, data_processor, trainer)\n",
    "\n",
    "    # 2. Predict single tweet\n",
    "    single_result = predict_single_tweet(\n",
    "        \"NVIDIA's new GPU is amazing!\",\n",
    "        platform=\"Twitter\"\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nSingle Tweet Prediction:\")\n",
    "    print(f\"Tweet: {single_result['tweet']}\")\n",
    "    print(f\"Predicted Sentiment: {single_result['predicted_sentiment']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Example usage error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
